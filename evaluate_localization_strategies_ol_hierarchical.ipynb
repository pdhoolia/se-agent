{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate localization strategies\n",
    "\n",
    "This notebook does a comparative evaluation of different localization strategies.\n",
    "- Defines a base interface for localization\n",
    "- Implements a few localization strategies\n",
    "- Defines an evaluator that runs a test suite on those localization strategies\n",
    "- Evaluator dumps the results in a pandas dataframe\n",
    "- Uses Milvus as the vector database\n",
    "- Uses OpenAI's embeddings model\n",
    "- Uses langchain's abstractions for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from se_agent.localizer import localize_issue\n",
    "from se_agent.project import Project\n",
    "from se_agent.project_manager import ProjectManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base interface for localization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy(ABC):\n",
    "    @abstractmethod\n",
    "    def localize(self, issue: Dict[str, str], top_n: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Localizes the issue to a set of relevant packages and files.\n",
    "\n",
    "        Args:\n",
    "            issue (Dict[str, str]): A dictionary containing issue details with at least:\n",
    "                - `title` (str): The title of the issue.\n",
    "                - `description` (str): The detailed description of the issue.\n",
    "            top_n (int): The maximum number of localization results to return.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, str]]: A list of tuples representing relevant localization results,\n",
    "                each containing `package` (str) and `file` (str).\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical localization strategy\n",
    "\n",
    "Instead of semantic vector search, this strategy uses the completion API to generate localization results. This requires inlining the context. Using all the files in the repository as context, far-exceed the permitted token limits of the completion API. Therefore, it uses generated semantic summaries of the code files as context. However, for large repositories, and depending on the model used, this may still exceed the token limits. Therefore, it also generates higher-level summaries at the level of packages. Let us assume that the aggregated package summaries are within the token limits. The strategy operates as follows:\n",
    "\n",
    "- **Package level**: Given an issue, it first identifies the package that are relevant to the issue query belongs to, using packages summaries in the inline context.\n",
    "- **File level**: It then identifies the files within the package that are relevant to the issue query, using file summaries for the relevant packages in the inline context.\n",
    "\n",
    "This strategy is more expensive than the semantic vector search strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLocalizationStrategy(Strategy):\n",
    "    def __init__(self, project: Project, strategy_name: str = \"Hierarchical Completion\"):\n",
    "        self.project = project\n",
    "        self.strategy_name = strategy_name\n",
    "\n",
    "    def localize(self, issue: Dict[str, str], top_n: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Localizes an issue to specific files by first identifying relevant packages\n",
    "        and then narrowing down to specific files in those packages.\n",
    "        \"\"\"\n",
    "        # issue conversation\n",
    "        issue_conversation = {\n",
    "            \"title\": issue[\"title\"],\n",
    "            \"conversation\": [{'role': 'user', 'content': f'Issue: {issue[\"title\"]}\\n\\nDescription: {issue[\"description\"]}'}]\n",
    "        }\n",
    "\n",
    "        # Localize the issue using the hierarchical approach\n",
    "        localization_suggestions = localize_issue(self.project, issue, issue_conversation)\n",
    "\n",
    "        if localization_suggestions is None:\n",
    "            return []  # If localization fails, return an empty list\n",
    "\n",
    "        # Format the results as (package, file) tuples, sorted by confidence\n",
    "        return [(suggestion.package, os.path.splitext(suggestion.file)[0]) for suggestion in localization_suggestions[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterator\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "class Issue:\n",
    "    def __init__(self, id: str, title: str, content: str, expected_results: List[str]):\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.expected_results = expected_results\n",
    "\n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        \"\"\"Returns the issue data as a dictionary for easy access.\"\"\"\n",
    "        return {\"title\": self.title, \"description\": self.content}\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, yaml_path: str):\n",
    "        self.yaml_dir = os.path.dirname(yaml_path)  # Get the directory containing the YAML file\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        self.test_cases = data[\"test_cases\"]\n",
    "\n",
    "    def __iter__(self) -> Iterator[Issue]:\n",
    "        \"\"\"Allows iteration over Issue instances created from test cases.\"\"\"\n",
    "        for case in self.test_cases:\n",
    "            # Construct the full path to the markdown file\n",
    "            full_path = os.path.join(self.yaml_dir, case[\"filepath\"])\n",
    "            # Load the content from the markdown file\n",
    "            with open(full_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            # Create an Issue instance for each test case\n",
    "            yield Issue(\n",
    "                id=case[\"id\"],\n",
    "                title=case[\"title\"],\n",
    "                content=content,\n",
    "                expected_results=case[\"expected_results\"]\n",
    "            )\n",
    "\n",
    "dataset = Dataset(\"test/dataset.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalizationEvaluator:\n",
    "    def __init__(self, dataset: Dataset, strategies_to_evaluate: List[Strategy]):\n",
    "        self.dataset = dataset\n",
    "        self.strategies = strategies_to_evaluate\n",
    "\n",
    "    def calculate_score(self, expected_results: List[str], actual_results: List[str]) -> float:\n",
    "        \"\"\"Calculates the score with distance-based penalties for expected results outside the top-k.\"\"\"\n",
    "        score = 1.0  # Start with a perfect score of 1\n",
    "\n",
    "        for expected in expected_results:\n",
    "            if expected in actual_results:\n",
    "                index = actual_results.index(expected)\n",
    "                # Check if expected item is within the top-k\n",
    "                if index >= len(expected_results):\n",
    "                    # Distance-based partial penalty if it's outside top-k but present in results\n",
    "                    distance_factor = index - len(expected_results) + 1\n",
    "                    penalty = (1 / len(expected_results)) * distance_factor * 0.2\n",
    "                    score -= penalty\n",
    "            else:\n",
    "                # Full penalty if expected item is missing altogether\n",
    "                score -= 1 / len(expected_results)\n",
    "\n",
    "        return max(score, 0)  # Ensure score doesn't go below 0\n",
    "\n",
    "    def evaluate(self) -> pd.DataFrame:\n",
    "        \"\"\"Evaluates each strategy on all test issues and returns a DataFrame with results and scores.\"\"\"\n",
    "        df = pd.DataFrame(columns=[\"Issue Title\", \"Expected Results\"] + [f\"Results ({strategy.strategy_name})\" for strategy in self.strategies])\n",
    "\n",
    "        # Dictionary to store total scores per strategy\n",
    "        total_scores = {strategy.strategy_name: 0 for strategy in self.strategies}\n",
    "\n",
    "        # Iterate over each Issue in the dataset\n",
    "        for issue in self.dataset:\n",
    "            issue_data = {\"title\": issue.title, \"description\": issue.content}  # Prepare data for localization\n",
    "            row_data = {\n",
    "                \"Issue Title\": issue.title,\n",
    "                \"Expected Results\": issue.expected_results\n",
    "            }\n",
    "\n",
    "            # Calculate and store results and formatted score+results for each strategy\n",
    "            for strategy in self.strategies:\n",
    "                actual_results = [res[1] for res in strategy.localize(issue_data, top_n=5)]\n",
    "                score = self.calculate_score(issue.expected_results, actual_results)\n",
    "                total_scores[strategy.strategy_name] += score  # Accumulate score for total\n",
    "\n",
    "                # Format results with score as requested\n",
    "                formatted_result = f\"{score:.2f} {actual_results}\"\n",
    "                row_data[f\"Results ({strategy.strategy_name})\"] = formatted_result\n",
    "\n",
    "            # Append row data to DataFrame\n",
    "            df = pd.concat([df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "\n",
    "        # Append total scores row to DataFrame\n",
    "        total_row = {\"Issue Title\": \"Total\", \"Expected Results\": \"\"}\n",
    "        for strategy in self.strategies:\n",
    "            total_row[f\"Results ({strategy.strategy_name})\"] = f\"{total_scores[strategy.strategy_name]:.2f}\"\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_store = \"/Users/pdhoolia/projects-store\"\n",
    "repo_full_name = \"pdhoolia/se-agent\"\n",
    "src_dir = \"se_agent\"\n",
    "\n",
    "project_manager = ProjectManager(projects_store)\n",
    "project_info = project_manager.get_project(repo_full_name)\n",
    "project = Project(os.getenv(\"GITHUB_TOKEN\"), projects_store, project_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_strategy = HierarchicalLocalizationStrategy(project, strategy_name=\"Hierarchical Localization\")\n",
    "strategies_to_evaluate = [hierarchical_strategy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = LocalizationEvaluator(\n",
    "    dataset=dataset,\n",
    "    strategies_to_evaluate=strategies_to_evaluate\n",
    ")\n",
    "\n",
    "evaluation_results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame for display purposes\n",
    "display_df = evaluation_results.copy()\n",
    "\n",
    "# Set the index to start from 1\n",
    "display_df.index = display_df.index + 1\n",
    "\n",
    "# Apply left alignment to all columns, including headers\n",
    "df_style = display_df.style \\\n",
    "    .set_table_attributes(\"style='width:100%'\") \\\n",
    "    .set_properties(**{'text-align': 'left'}) \\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'th',\n",
    "        'props': [('text-align', 'left')]\n",
    "    }])\n",
    "\n",
    "df_style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
