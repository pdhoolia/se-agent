# @server_url = https://dhoolia.ngrok.io
# @server_url = https://dhoolia.com
@server_url = http://localhost:3000
# @server_url = http://se-agent.sl.cloud9.ibm.com:3000

### Onboard: New Project: GitHub Public
POST {{server_url}}/onboard HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "repo_full_name": "conversational-ai/se-agent",
    "src_folder": "se_agent",
    "main_branch": "main",
    "api_url": "https://github.ibm.com/api/v3"
}

### Onboard / Refresh: Existing Project: GitHub Public
PUT {{server_url}}/onboard HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "repo_full_name": "pdhoolia/se-agent",
    "src_folder": "se_agent",
    "main_branch": "main"
}

### Onboard: New Project: GHE
POST {{server_url}}/onboard HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "repo_full_name": "conversational-ai/se-agent",
    "src_folder": "se_agent",
    "main_branch": "main",
    "api_url": "https://github.ibm.com/api/v3"
}


### Onboard / Refresh: Existing Project: GHE
PUT {{server_url}}/onboard HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "repo_full_name": "conversational-ai/se-agent",
    "src_folder": "se_agent",
    "main_branch": "main",
    "api_url": "https://github.ibm.com/api/v3"
}

### Test Issue Creation Event
POST {{server_url}}/webhook HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "action": "opened",
    "issue": {
        "number": 58,
        "title": "Process only issues notifying the se-agent",
        "body": "Process issue / issue comment events only if title or description mentions / notifies se-agent (e.g., by including \"@Se-agent\" in the title or description of the issue.",
        "state": "open"
    },
    "repository": {
        "full_name": "conversational-ai/se-agent"
    }
}

### Test Issue Creation Event
POST {{server_url}}/webhook HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "action": "opened",
    "issue": {
        "number": 100,
        "title": "Tolerate inconsistencies in files localization",
        "body": "In `HierarchicalLocalizationStrategy.localize(...)`:\n\n1. let's add fuzziness in mapping LLM file localization suggestions to actual file path determination logic. Some examples of such fuzziness are:\n\n    - Say LLM returns a localization suggestion with package as `x/y.py` or `x.y` (which are incorrect) and file as `y.py` which is correct, we should be able to look for the file in our module_src folder and return the correct relative filepath from repo folder.",
        "state": "open"
    },
    "repository": {
        "full_name": "conversational-ai/se-agent"
    }
}

### Test Issue Creation Event : langgraph-mcp
POST {{server_url}}/webhook HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "action": "opened",
    "issue": {
        "number": 100,
        "title": "Add `component_schema` to `mcp_orchestrator` node processing for `openapi-mcp-server`",
        "body": "`openapi-mcp-server` is a generic mcp server that dynamically enables an mcp-server wrapper for the `open-api-specification` document specified as an input to it.\n\nHere's a sample of its `server_config`:\n\n```json\n\"petstore-api\": {\n    \"command\": \"npx\",\n    \"args\": [\"openapi-mcp-server\", \"/path/to/petstore-openapi.json\"]\n}\n```\n\nServer configurations for many open-api-specifications may be plugged in using the generic `openapi-mcp-server`. To recognize if a server is an `openapi-mcp-server` type:\n\n- args[0] should be `openapi-mcp-server`\n- args[1] would be the path to the open-api-specification being empowered via mcp by openapi-mcp-server\n\nOne common problem observed while using `openapi-mcp-server` is that the operations in the open-api-specification may use references to the component-schema for the arguments. In such cases the tools returned by `get_tools` have schema reference instead of the actual schema. That makes it difficult for the LLM used in `mcp_orchestrator` node in the `assistant_graph.py` to map arguments from the conversation messages for the tool_calls.\n\nWe may address this as follows:\n\n1. Modify the `MCP_ORCHESTRATOR_SYSTEM_PROMPT` to add a variable `{schema_component}` right at the end (but preferably before system time).\n\nTo populate `schema_component` in the `mcp_orchestrator` node:\n\n1. read the openapi-specification (in case current mcp server is of `openapi-mcp-server` type.\n2. create `schema_component` string with something like:\n   ```txt\n   If any of the tools have reference to the component-schema, use the following to understand the structure of its arguments: {component_schema}\n3. Add it to the LLM prompt being used by `mcp_orchestrator` node.",
        "state": "open"
    },
    "repository": {
        "full_name": "conversational-ai/langgraph-mcp"
    }
}

### Test Issue Creation Event : mlx-playground
POST {{server_url}}/webhook HTTP/1.1
Content-Type: application/json
Accept: application/json

{
    "action": "opened",
    "issue": {
        "number": 100,
        "title": "Change train, validate, test split to 60-20-20",
        "body": "Currently, the train, validate, test split is 80-10-10. Let's change it to 60-20-20.",
        "state": "open"
    },
    "repository": {
        "full_name": "conversational-ai/mlx-playground"
    }
}
