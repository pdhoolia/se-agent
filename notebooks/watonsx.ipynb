{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watsonx.ai with langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "You are an AI assistant that helps with software issue localization.\n",
    "\n",
    "Following package summaries are available for your reference:\n",
    "\n",
    "[PACKAGE-SUMMARIES-START]\n",
    "# llm\n",
    "\n",
    "## Semantic Summary\n",
    "The `llm` package provides a comprehensive interface for interacting with the OpenAI API to generate semantic descriptions, package summaries, and suggestions using language models. It is designed to handle API interactions with robust retry mechanisms and exponential backoff strategies. Additionally, it includes utilities for creating tailored prompts for analyzing Python code, identifying relevant packages, localizing issues, and suggesting code changes. This package is particularly useful for developers seeking to automate and enhance their code analysis and documentation processes.\n",
    "\n",
    "## Contained code structure names\n",
    "`__init__.py`, `api.py`, `prompts.py`, `call_llm_to_generate_semantic_description`, `call_llm_to_generate_package_summary`, `call_llm_for_localization`, `call_llm_for_suggestions`, `retry_with_exponential_backoff`, `completions_with_backoff`, `parsed_completions_with_backoff`, `call_llm`, `prompt_generate_semantic_description`, `prompt_generate_package_summary`, `prompt_identify_relevant_packages`, `prompt_localize_to_files`, `prompt_generate_change_suggestions`\n",
    "\n",
    "# repository_analyzer\n",
    "\n",
    "## Semantic Summary\n",
    "The `repository_analyzer` package provides tools for analyzing code repositories by generating semantic descriptions and summaries. It leverages language model APIs to process code files and documentation, enabling users to extract meaningful insights and summaries from complex codebases. The package focuses on two main functionalities: creating semantic descriptions of individual code files and generating comprehensive summaries for entire packages based on their documentation.\n",
    "\n",
    "## Contained code structure names\n",
    "`__init__.py`, `file_analyzer.py`, `package_summary.py`, `generate_semantic_description`, `generate_package_summary`\n",
    "\n",
    "# se_agent\n",
    "\n",
    "## Semantic Summary\n",
    "The `se_agent` package is a comprehensive software engineering tool designed to facilitate the management and enhancement of GitHub projects. It integrates with GitHub to automate the process of analyzing issues, suggesting code changes, and updating the understanding of a project’s codebase. The package features a webhook listener for real-time interactions, an issue analyzer, and a localization mechanism to pinpoint relevant code files. Additionally, it includes functionalities for onboarding projects by cloning and managing repositories, generating semantic documentation, and interacting with GitHub’s API for seamless project management.\n",
    "\n",
    "## Contained code structure names\n",
    "`__init__.py`, `change_suggester.py`, `github_listener.py`, `issue_analyzer.py`, `localizer.py`, `main.py`, `onboard_agent.py`, `project.py`, `project_info.py`, `project_manager.py`, `suggest_changes`, `get_project_manager`, `webhook`, `process_issue_event`, `run_server`, `IGNORE_TOKEN`, `logger`, `analyze_issue`, `RelevantPackages`, `FileLocalizationSuggestion`, `FileLocalizationSuggestions`, `localize_issue`, `main`, `Project`, `ProjectInfo`, `ProjectManager`, `onboard_agent`, `load_checkpoint`, `save_checkpoint`, `delete_checkpoint`, `clone_repository`, `pull_latest_changes`, `update_codebase_understanding`, `onboard`, `create_hierarchical_document`, `fetch_package_summaries`, `fetch_package_details`, `fetch_code_files`, `post_issue_comment`, `fetch_issue_comments`, `add_project`, `get_project`, `list_projects`.\n",
    "\n",
    "# util\n",
    "\n",
    "## Semantic Summary\n",
    "The `util` package provides utility scripts for file and folder management within a directory. It includes functionality to count files and folders recursively, with options for filtering by file extension. The package is designed to handle common errors such as missing directories and offers a command-line interface for user interaction, making it convenient for integrating into various workflows.\n",
    "\n",
    "## Contained code structure names\n",
    "`__init__.py`, `file_count.py`, `folder_count.py`, `count_files_in_folder_recursive`, `count_folders_in_folder_recursive`\n",
    "[PACKAGE-SUMMARIES-END]\n",
    "\n",
    "You understand the issues raised and discussed by the user.\n",
    "Analyze any code snippets provided.\n",
    "Based on the package summaries above, identify the packages most relevant to the discussion.\n",
    "And finally, return a list of high-level packages (as a JSON array) that you think are most relevant for the issue and discussion.\n",
    "\n",
    "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "Here is the output schema:\n",
    "```\n",
    "{{\"properties\": {{\"relevant_packages\": {{\"items\": {{\"type\": \"string\"}}, \"title\": \"Relevant Packages\", \"type\": \"array\"}}}}, \"required\": [\"relevant_packages\"]}}\n",
    "```\n",
    "\"\"\"\n",
    "    },\n",
    "    {   \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "Issue: lets move to llm agnostic implementation\n",
    "\n",
    "Description: Our llm interface implemented in api.py is currently openai specific.\n",
    "Let's transform that to llm provider agnostic implementation.\n",
    "Let's use langchain for that.\n",
    "\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output strcture expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class RelevantPackages(BaseModel):\n",
    "    relevant_packages: list[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "watsonx env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "apikey = os.getenv(\"WATSONX_APIKEY\")\n",
    "project_id = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "url = os.getenv(\"WATSONX_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a chat model using IBM watsonx.ai and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import ChatWatsonx\n",
    "\n",
    "chat_model = ChatWatsonx(\n",
    "    model_id=\"mistralai/mistral-large\",\n",
    "    url=url,\n",
    "    apikey=apikey,\n",
    "    project_id=project_id,\n",
    "    params={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 512,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to transform test conversation to the format expected by the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    ")\n",
    "\n",
    "def transform_to_langchain_message_format(messages):\n",
    "    \"\"\"Transforms messages to Langchain chat prompt template format.\"\"\"\n",
    "    transformed_messages = []\n",
    "    for message in messages:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        # Create corresponding Langchain message object based on role\n",
    "        if role == 'user':\n",
    "            transformed_message = HumanMessage(content=content)\n",
    "        elif role == 'assistant':\n",
    "            transformed_message = AIMessage(content=content)\n",
    "        elif role == 'system':\n",
    "            transformed_message = SystemMessage(content=content)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role: {role}\")\n",
    "\n",
    "        transformed_messages.append(transformed_message)\n",
    "    return transformed_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelevantPackages(relevant_packages=['llm'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.with_structured_output(RelevantPackages).invoke(transform_to_langchain_message_format(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a language model using IBM watsonx.ai and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "language_model = WatsonxLLM(\n",
    "\tmodel_id=\"meta-llama/llama-3-405b-instruct\",\n",
    "\tproject_id=project_id,\n",
    "    url=url,\n",
    "    apikey=apikey,\n",
    "    params={\n",
    "\t\t\"decoding_method\": \"greedy\",\n",
    "\t\t\"max_new_tokens\": 900,\n",
    "\t\t\"repetition_penalty\": 1\n",
    "\t},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to transform test conversation to the format expected by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_single_prompt_string(messages):\n",
    "    \"\"\"Transforms a list of messages to a single prompt string.\"\"\"\n",
    "    prompt = \"<|begin_of_text|>\"\n",
    "    for message in messages:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        prompt += f\"<|start_header_id|>{role}<|end_header_id|>{content}<|eot_id|>\"\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nBased on the issue and discussion, I have analyzed the code snippets and package summaries provided. The issue revolves around transforming the current OpenAI-specific LLM interface in `api.py` to an LLM provider-agnostic implementation using LangChain.\\n\\nThe most relevant packages to this discussion are:\\n\\n* `llm`: This package currently contains the OpenAI-specific LLM interface in `api.py`, which needs to be transformed.\\n* `langchain`: Although not listed in the provided package summaries, LangChain is mentioned in the discussion as the tool to be used for the LLM provider-agnostic implementation.\\n\\nHere is the list of high-level packages that I think are most relevant for the issue and discussion:\\n\\n```\\n{\"relevant_packages\": [\"llm\"]}\\n```\\n\\nNote that `langchain` is not included in the output as it is not part of the provided package summaries. However, it is understood to be a crucial component in the solution.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model.invoke(input=transform_to_single_prompt_string(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.with_structured_output(RelevantPackages).invoke(input=transform_to_single_prompt_string(messages))\n",
    "\n",
    "# =>\n",
    "#   NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can post process using PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=RelevantPackages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a pipeline with llm and parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = language_model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now invoke the chain with the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelevantPackages(relevant_packages=['llm'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input=transform_to_single_prompt_string(messages))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
