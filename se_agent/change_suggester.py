"""Module for generating code change suggestions based on issue analysis and relevant code files."""

import os
import logging
from typing import List

logger = logging.getLogger("se-agent")

from se_agent.llm.api import call_llm_for_task
from se_agent.llm.model_configuration_manager import TaskName
from se_agent.project import Project

TOP_N_FILES = int(os.getenv('TOP_N_FILES', 3))


def prompt_generate_change_suggestions(issue_analysis, file_suggestions, code_files):
    """Generates a prompt for the LLM to localize the issue and suggest code changes.

    Args:
        issue_analysis (dict): The analysis results of the issue, including conversation.
        file_suggestions (str): A string representation of the suggested file paths.
        code_files (str): The code contents of the suggested files.

    Returns:
        List[dict]: A list of messages formatted for the LLM.
    """
    messages = []

    # System message with instructions and relevant code files
    system_message = {
        'role': 'system',
        'content': f"""You are an AI assistant that specializes in analysing issues and understanding code, and make code change suggestions to address issues.

Following files have been suggested as relevant to the issue and discussion:

[FILE-SUGGESTIONS-START]
{file_suggestions}
[FILE-SUGGESTIONS-END]

Here are the corresponding code files:
{code_files}

Based on the issue details and ensuing discussion please suggest code or changes to it in these files and (or any new code) along with your reasoning. In case of code change, don't write the entire code or function. Focus on just the relevant parts that are changed."""
    }
    messages.append(system_message)

    # Add conversation messages to the prompt
    conversation = issue_analysis.get('conversation', [])
    for message in conversation:
        # Map roles appropriately
        role = 'user' if message['role'] == 'user' else 'assistant'
        messages.append({'role': role, 'content': message['content']})

    return messages


def suggest_changes(project: Project, analysis_results: dict, filepaths: List[str]) -> str:
    """Suggests code changes based on issue analysis and relevant files.

    Args:
        project (Project): The project instance containing configurations.
        analysis_results (dict): The results from analyzing the issue.
        filepaths (List[str]): List of file paths relevant to the issue.

    Returns:
        str: The change suggestions generated by the LLM, or None if an error occurs.
    """
    # Determine the number of top files to include
    top_n_files = project.info.top_n_files if project.info.top_n_files is not None else TOP_N_FILES

    # Fetch contents of the suggested files
    file_contents = project.fetch_code_files(filepaths[:top_n_files])

    # Log the file paths being added to the prompt
    logger.debug(f"Files being added to the prompt: {filepaths[:top_n_files]}")

    # Build the code_files part of the prompt from the file contents
    code_files = ""
    for filepath, file_content in zip(filepaths[:top_n_files], file_contents):
        code_files += f"""
file: {filepath}
{file_content}
"""

    # Generate the prompt for change suggestions
    messages = prompt_generate_change_suggestions(analysis_results, str(filepaths), code_files)

    # Call the LLM to generate change suggestions
    try:
        change_suggestions_response = call_llm_for_task(
            task_name=TaskName.GENERATE_SUGGESTIONS,
            messages=messages
        ).content
    except Exception as e:
        print(f"Error calling LLM for change suggestions: {e}")
        return None

    return change_suggestions_response